


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import missingno as msno
import re
import time
import folium
import warnings
import nltk
import spacy

from collections import defaultdict
from folium.plugins import MarkerCluster
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError
from scipy.stats import gaussian_kde
from scipy import stats
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.decomposition import LatentDirichletAllocation
from bs4 import BeautifulSoup
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from datetime import datetime



from tqdm.notebook import tqdm
%matplotlib inline

warnings.filterwarnings("ignore", category=UserWarning, module='bs4')
############################################################################################################################################
#########################################           Data Understanding and Preparation          ############################################
############################################################################################################################################
# Data Profiling
df = pd.read_csv('profiles.csv')
df.head()


%%bash
pip install numpy==1.24.3 pandas==1.5.3



df.info()


df.describe(include='all')


numeric_cols = ['age', 'height', 'income']
fig, axes = plt.subplots(len(numeric_cols), 2, figsize=(14, 4*len(numeric_cols)))

for i, col in enumerate(numeric_cols):
    if col in df.columns:
        # Histogram
        sns.histplot(df[col].dropna(), kde=True, ax=axes[i, 0])
        axes[i, 0].set_title(f'Distribution of {col}')
        
        # Boxplot
        sns.boxplot(y=df[col].dropna(), ax=axes[i, 1])
        axes[i, 1].set_title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()


numeric_df = df[numeric_cols].dropna()
sns.pairplot(numeric_df)
plt.suptitle('Relationships Between Numeric Variables', y=1.02)
plt.show()


categorical_cols = ['sex', 'orientation', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'job', 'status', 'offspring', 'pets', 'speaks']
plt.figure(figsize=(12, 10))

for col in categorical_cols:
    if col in df.columns:
        # Check the number of unique values
        n_unique = df[col].nunique()
        
        if n_unique > 15:  # Too many categories
            print(f"Skipping {col}: too many categories ({n_unique})")
            continue
            
        plt.figure(figsize=(10, 6))
        value_counts = df[col].value_counts().sort_values(ascending=False)
        
        # Plot top categories
        sns.barplot(x=value_counts.index, y=value_counts.values, palette='viridis')
        plt.title(f'Distribution of {col}')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()



essay_cols = [col for col in df.columns if col.startswith('essay')]
essay_lengths = pd.DataFrame()

for col in essay_cols:
    # Calculate non-null percentage
    non_null_pct = (df[col].notnull().mean() * 100)
    # Calculate average length of text
    df[f'{col}_length'] = df[col].fillna('').astype(str).apply(len)
    
    print(f"{col}: {non_null_pct:.1f}% filled, avg length: {df[f'{col}_length'].mean():.1f} chars")

# Visualize essay completion rates
plt.figure(figsize=(10, 6))
completion_rates = pd.Series({col: df[col].notnull().mean() * 100 for col in essay_cols})
sns.barplot(x=completion_rates.index, y=completion_rates.values)
plt.title('Essay Completion Rates')
plt.ylabel('Percentage Filled (%)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# Function to get coordinates with error handling and rate limiting
def get_coordinates(location_name, geolocator, delay=1):
    """
    Get latitude and longitude for a location with error handling and rate limiting
    """
    try:
        time.sleep(delay)  # Rate limiting
        if pd.isna(location_name) or location_name == '':
            return None, None
        
        location = geolocator.geocode(location_name)
        if location:
            return location.latitude, location.longitude
        else:
            return None, None
    except (GeocoderTimedOut, GeocoderServiceError):
        return None, None
    

# Create a sample of the data to avoid too many geocoding requests
# In a real project, you'd save these results to avoid repeated API calls
location_sample = df['location'].dropna().sample(n=min(100, len(df['location'].dropna())))

# Initialize geocoder
geolocator = Nominatim(user_agent="okcupid_project")

# Create DataFrame to store coordinates
geo_df = pd.DataFrame(location_sample)
geo_df['latitude'] = None
geo_df['longitude'] = None

# Geocode the locations
print("Geocoding locations (this may take a while due to rate limiting)...")
for idx, row in tqdm(geo_df.iterrows(), total=len(geo_df)):
    lat, lon = get_coordinates(row['location'], geolocator)
    geo_df.at[idx, 'latitude'] = lat
    geo_df.at[idx, 'longitude'] = lon

# Remove rows with failed geocoding
geo_df = geo_df.dropna(subset=['latitude', 'longitude'])

# Display the results
print(f"Successfully geocoded {len(geo_df)} out of {len(location_sample)} locations")
geo_df.head()

# Create a map to visualize the locations
map_center = [geo_df['latitude'].mean(), geo_df['longitude'].mean()]
mymap = folium.Map(location=map_center, zoom_start=4)

# Add a marker cluster to handle many points more efficiently
marker_cluster = MarkerCluster().add_to(mymap)

# Add markers for each location
for idx, row in geo_df.iterrows():
    popup_text = f"Location: {row['location']}"
    folium.Marker(
        location=[row['latitude'], row['longitude']],
        popup=popup_text
    ).add_to(marker_cluster)

# Display the map
mymap


# Calculate location density using kernel density estimation
if len(geo_df) > 5:  # Need sufficient points for meaningful KDE
    
    # Ensure longitude and latitude are numeric
    geo_df['longitude'] = pd.to_numeric(geo_df['longitude'], errors='coerce')
    geo_df['latitude'] = pd.to_numeric(geo_df['latitude'], errors='coerce')
    
    # Drop rows with NaN values after conversion
    geo_df = geo_df.dropna(subset=['longitude', 'latitude'])
    
    # Create a KDE of the locations
    x = geo_df['longitude']
    y = geo_df['latitude']
    k = gaussian_kde([x, y])
    xi, yi = np.mgrid[x.min():x.max():100j, y.min():y.max():100j]
    zi = k(np.vstack([xi.flatten(), yi.flatten()]))
    
    # Plot the KDE
    plt.figure(figsize=(12, 8))
    plt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto', cmap='viridis')
    plt.colorbar(label='Density')
    plt.scatter(x, y, s=5, alpha=0.3)
    plt.title('User Location Density')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.tight_layout()
    plt.show()


# Data Cleaning Strategy
# Calculate percentage of null values in each column
missing_data = df.isnull().mean().sort_values(ascending=False)

# Plot the missing data percentages
plt.figure(figsize=(12, 8))
sns.barplot(x=missing_data.index, y=missing_data.values)
plt.xticks(rotation=90)
plt.ylabel('Null values density')
plt.title('Null values distribution')
plt.tight_layout()
plt.show()



plt.figure()
msno.matrix(df)
plt.savefig('missing_values2.png')
msno.heatmap(df)
plt.savefig('missing_values.png')
plt.show()


def check_mar_correlations(df):
    """Check correlations between values and missingness indicators"""
    # Select only numeric columns for correlation analysis
    numeric_df = df.select_dtypes(include=[np.number])
    
    # Create binary indicators for missingness
    missing_indicators = df.isna().astype(int)
    
    # Add '_missing' suffix to column names
    missing_indicators.columns = [f"{col}_missing" for col in df.columns]
    
    # Combine numeric data with missing indicators
    combined = pd.concat([numeric_df, missing_indicators], axis=1)
    
    # Calculate correlations
    corr_matrix = combined.corr()
    
    # Find significant correlations between missingness and values
    significant_correlations = []
    
    for col in numeric_df.columns:
        for miss_col in missing_indicators.columns:
            if col != miss_col.replace('_missing', ''):  # Don't compare a column with its own missingness
                if col in corr_matrix.index and miss_col in corr_matrix.columns:
                    corr_value = corr_matrix.loc[col, miss_col]
                    if not np.isnan(corr_value) and abs(corr_value) > 0.3:  # Threshold for meaningful correlation
                        significant_correlations.append({
                            'value_column': col,
                            'missing_column': miss_col,
                            'correlation': corr_value
                        })
    
    return pd.DataFrame(significant_correlations).sort_values('correlation', ascending=False)

def logistic_regression_test(df, target_column):
    """Test if missingness in target_column can be predicted by other variables"""
    
    # Skip if column has no missing values
    if df[target_column].isna().sum() == 0:
        return {'is_predictable': False, 'message': 'No missing values in column'}
    
    # Create a binary indicator for missingness in the target column
    missing_indicator = df[target_column].isna().astype(int)
    
    # Create features from other columns, imputing missing values
    features = df.drop(columns=[target_column])
    
    # Skip if no features available
    if features.shape[1] == 0:
        return {'is_predictable': False, 'message': 'No features available'}
    
    # Impute missing values in features
    imputer = SimpleImputer(strategy='median')
    try:
        features_imputed = pd.DataFrame(
            imputer.fit_transform(features),
            columns=features.columns
        )
    except:
        return {'is_predictable': False, 'message': 'Failed to impute values'}
    
    # Fit logistic regression
    model = LogisticRegression(max_iter=1000, solver='liblinear')
    try:
        model.fit(features_imputed, missing_indicator)
        
        # Calculate scores
        score = model.score(features_imputed, missing_indicator)
        
        # Get feature importances
        coefficients = pd.DataFrame({
            'feature': features.columns,
            'coefficient': model.coef_[0]
        }).sort_values('coefficient', ascending=False)
        
        return {
            'score': score,
            'coefficients': coefficients,
            'is_predictable': score > 0.7,  # Threshold suggesting MAR
            'message': f'Model score: {score:.3f}'
        }
    except Exception as e:
        return {'is_predictable': False, 'message': f'Model fitting failed: {str(e)}'}

def manual_mcar_test(df):
    """
    Manual implementation to test MCAR hypothesis
    This checks if missingness in one variable is related to values in other variables
    """
    # Create missingness indicators
    missing_indicators = df.isna().astype(int)
    
    # Only test numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # For each pair of columns, test if missingness in one is related to values in another
    results = []
    
    for col1 in df.columns:
        # Only test columns with sufficient missing values
        if missing_indicators[col1].sum() < 10 or missing_indicators[col1].sum() > len(df) - 10:
            continue
            
        for col2 in numeric_cols:
            if col1 != col2:
                # Split data based on whether col1 is missing
                missing_in_col1 = missing_indicators[col1] == 1
                present_in_col1 = ~missing_in_col1
                
                # Get values of col2 where col1 is missing vs present
                values_when_col1_missing = df.loc[missing_in_col1, col2].dropna()
                values_when_col1_present = df.loc[present_in_col1, col2].dropna()
                
                # If enough data to compare
                if len(values_when_col1_missing) > 5 and len(values_when_col1_present) > 5:
                    try:
                        # t-test to see if the distributions differ
                        t_stat, p_val = stats.ttest_ind(
                            values_when_col1_missing,
                            values_when_col1_present,
                            equal_var=False  # Welch's t-test
                        )
                        
                        results.append({
                            'missing_col': col1,
                            'value_col': col2,
                            't_statistic': t_stat,
                            'p_value': p_val,
                            'significant': p_val < 0.05
                        })
                    except:
                        pass  # Skip if test fails
    
    result_df = pd.DataFrame(results)
    
    # If we have results, summarize them
    if len(result_df) > 0:
        significant_tests = result_df[result_df['significant'] == True]
        
        print(f"Performed {len(result_df)} tests of MCAR pattern")
        print(f"Found {len(significant_tests)} significant relationships between missingness and values")
        
        if len(result_df) > 0:
            print(f"Proportion significant: {len(significant_tests) / len(result_df):.2%}")
        
        if len(significant_tests) > 0:
            print("\nTop relationships between missingness and values (challenging MCAR assumption):")
            print(significant_tests.sort_values('p_value').head(10))
            
            mcar_likelihood = 1 - (len(significant_tests) / len(result_df))
            print(f"\nLikelihood data is MCAR: {mcar_likelihood:.2%}")
            
            if mcar_likelihood > 0.95:
                print("Data appears to be MCAR (Missing Completely At Random)")
            elif mcar_likelihood > 0.8:
                print("Data may be MCAR, but there are some patterns challenging this assumption")
            else:
                print("Data is likely not MCAR - could be MAR or MNAR")
        else:
            print("No significant relationships found. Data may be MCAR.")
            
        return result_df
    else:
        print("Could not perform enough tests to determine MCAR pattern")
        return None

def missing_pattern_by_features(df, column_of_interest=None):
    """
    Examine if missing values are related to specific features
    If column_of_interest is provided, focuses on that column's missingness
    """
    if column_of_interest is None:
        # Count missing values per row
        missingness = df.isna().sum(axis=1)
        plt.figure(figsize=(10, 6))
        plt.hist(missingness, bins=30)
        plt.title('Missing Values Per Row')
        plt.xlabel('Number of Missing Values')
        plt.ylabel('Count of Rows')
        plt.show()
    else:
        # Focus on specified column's missingness
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        numeric_cols = [col for col in numeric_cols if col != column_of_interest]
        
        # Only proceed if we have numeric columns to analyze
        if not numeric_cols:
            print("No numeric columns available for analysis")
            return
            
        # Create indicator for missingness
        missing_indicator = df[column_of_interest].isna()
        
        # Test for differences in other columns based on missingness
        for col in numeric_cols[:5]:  # Limit to 5 columns for brevity
            plt.figure(figsize=(8, 6))
            sns.histplot(
                data=df, x=col, hue=missing_indicator,
                element="step", common_norm=False, stat="density",
                bins=20
            )
            plt.title(f'Distribution of {col} by {column_of_interest} Missingness')
            plt.show()
            
            # Calculate means
            mean_when_missing = df.loc[missing_indicator, col].mean()
            mean_when_present = df.loc[~missing_indicator, col].mean()
            print(f"Mean of {col} when {column_of_interest} is missing: {mean_when_missing:.2f}")
            print(f"Mean of {col} when {column_of_interest} is present: {mean_when_present:.2f}")
            
            # T-test
            try:
                t_stat, p_val = stats.ttest_ind(
                    df.loc[missing_indicator, col].dropna(),
                    df.loc[~missing_indicator, col].dropna()
                )
                print(f"T-test p-value: {p_val:.4f} {'(significant)' if p_val < 0.05 else ''}\n")
            except:
                print("Could not perform t-test\n")

def analyze_missing_patterns(df):
    """Run a comprehensive analysis of missing data patterns"""
    
    # Basic missing data summary
    missing_summary = pd.DataFrame({
        'missing_count': df.isna().sum(),
        'missing_percent': df.isna().sum() / len(df) * 100
    }).sort_values('missing_percent', ascending=False)
    
    print("Missing Data Summary:")
    print(missing_summary)
    
    # Visualize missing patterns
    try:
        import missingno as msno
        plt.figure(figsize=(12, 8))
        msno.matrix(df)
        plt.title('Missing Value Matrix')
        plt.show()
    except:
        print("Could not import missingno. Skipping visualization.")
    
    # Test for MCAR patterns
    print("\nTesting for MCAR patterns:")
    mcar_results = manual_mcar_test(df)
    
    # Check for MAR through correlations
    print("\nChecking for MAR through correlations:")
    mar_correlations = check_mar_correlations(df)
    if len(mar_correlations) > 0:
        print("Found significant correlations between values and missingness:")
        print(mar_correlations.head(10))  # Show top 10
        print("This suggests data may be MAR (Missing At Random)")
    else:
        print("No significant correlations found between values and missingness patterns")
    
    # For columns with substantial missing data, try logistic regression
    columns_to_test = missing_summary[missing_summary['missing_percent'] > 10].index.tolist()
    
    mar_evidence = []
    print("\nTesting if missingness can be predicted (MAR check):")
    for col in columns_to_test[:5]:  # Limit to first 5 columns with significant missingness
        print(f"Testing column: {col}")
        result = logistic_regression_test(df, col)
        print(f"  Result: {result.get('message', '')}")
        if result.get('is_predictable', False):
            print(f"  ✓ Missingness in '{col}' appears to be MAR - predictable from other variables")
            mar_evidence.append(col)
        else:
            print(f"  ✗ Missingness in '{col}' is not strongly predictable from other variables")
    
    # Summary
    print("\nSUMMARY:")
    if mar_evidence:
        print(f"- Found evidence of MAR in columns: {', '.join(mar_evidence)}")
    

analyze_missing_patterns(df)





numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
categorical_columns = df.select_dtypes(include=['object']).columns.tolist()

# FOR NUMERICAL VARIABLES
num_imputer = IterativeImputer(
    estimator=RandomForestRegressor(n_estimators=100, random_state=42),
    max_iter=10,
    random_state=42
    # Removed add_indicator=True which was causing the shape mismatch
)

# Create dictionary for column-specific constraints 
bounds = {
    'age': [18, 100],
    'height': [120, 220],  # in cm
    'income': [0, 250000]
}


# Apply the imputation with constraints
df_numeric = pd.DataFrame(
    num_imputer.fit_transform(df[numeric_columns]),
    columns=numeric_columns
)

# Apply bounds to ensure realistic imputed values
for col, (lower, upper) in bounds.items():
    if col in df_numeric.columns:
        df_numeric[col] = np.clip(df_numeric[col], lower, upper)

# FOR CATEGORICAL COLUMNS
essay_columns = [col for col in categorical_columns if col.startswith('essay')]
standard_cat_columns = [col for col in categorical_columns if not col.startswith('essay')]

# Encode the categorical variables
cat_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
encoded_cats = pd.DataFrame(
    cat_encoder.fit_transform(df[standard_cat_columns]),
    columns=standard_cat_columns
)

# Use KNN imputation for categorical data (after encoding)
cat_imputer = KNNImputer(n_neighbors=5, weights='distance')
imputed_cats = pd.DataFrame(
    cat_imputer.fit_transform(encoded_cats),
    columns=standard_cat_columns
)

df_categorical = pd.DataFrame(
    cat_encoder.inverse_transform(imputed_cats),
    columns=standard_cat_columns
)

# For essays, use a specialized approach
for col in essay_columns:
    # Leave truly empty essays as empty strings
    df_categorical[col] = df[col].fillna("")
    
    # prompt_texts = {"essay0": "About me:", "essay1": "What I'm doing with my life:", ...}
    # df_categorical[col] = df[col].fillna(prompt_texts.get(col, "No information provided"))

df_categorical = pd.concat([df_categorical, df[essay_columns].fillna("")], axis=1)

# Recombine the datasets
df_imputed = pd.concat([df_numeric, df_categorical], axis=1)


print(df_imputed.isnull().sum())


# Explore the unique values and their frequencies
for field in ['job', 'education']:
    print(f"\n{field.upper()} - Top 20 most common values:")
    value_counts = df_imputed[field].value_counts()
    print(value_counts.head(20))
    print(f"Total unique values: {len(value_counts)}")


def normalize_text(text):
    """Normalize text by converting to lowercase and removing special characters"""
    if pd.isna(text) or text == '':
        return None
    # Convert to lowercase and remove extra whitespace
    text = str(text).lower().strip()
    # Remove special characters
    text = re.sub(r'[^\w\s]', ' ', text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    return text

def standardize_job_categories(df):
    """Create standardized job categories from free-text job field"""
    # Create a copy of the original column
    df['job_original'] = df['job']
    df['job_standardized'] = df['job'].apply(normalize_text)
    
    # Create mappings for common job categories
    job_mappings = {
        # Technology
        'software|developer|programmer|engineer.*comput|web developer': 'software_engineer',
        'data.*scien|machine learning|ai|artificial intelligence': 'data_scientist',
        'system.*admin|network|it support|technical support': 'it_support',
        
        # Healthcare
        'doctor|physician|surgeon': 'medical_doctor',
        'nurse|nursing': 'nurse',
        'dentist|dental': 'dentist',
        'therap|psycholog|counsel': 'therapist',
        
        # Education
        'teacher|professor|instructor|lecturer': 'educator',
        'student': 'student',
        'researcher|scientist(?!.*data)': 'researcher',
        
        # Business
        'manag(?!.*project)|director|executive|ceo|cfo|coo': 'management',
        'sales|marketing': 'sales_marketing',
        'consult': 'consultant',
        'financ|accountant|bookkeep': 'finance',
        
        # Creative
        'design|graphic|artist|architect': 'designer_artist',
        'writer|editor|journalist': 'writer',
        'photograph': 'photographer',
        'music|musician': 'musician',
        
        # Service
        'retail|cashier|clerk|barista': 'retail',
        'chef|cook|baker|restaurant': 'food_service',
        'bartend|server|waiter|waitress': 'hospitality',
        
        # Other
        'unemployed|between jobs': 'unemployed',
        'self.*employed|entrepreneur|freelance': 'self_employed',
        'retired': 'retired',
        'rather not say|private|secret': 'undisclosed'
    }
    
    # Apply mappings
    standardized = df['job_standardized'].copy()
    for pattern, category in job_mappings.items():
        mask = standardized.str.contains(pattern, case=False, na=False, regex=True)
        standardized.loc[mask] = category
    
    # If any job wasn't matched by the patterns, keep it as 'other'
    standardized.fillna('other', inplace=True)
    
    # Update the dataframe
    df['job_category'] = standardized
    
    return df

def standardize_education_categories(df):
    """Create standardized education categories from free-text education field"""
    # Create a copy of the original column
    df['education_original'] = df['education']
    df['education_standardized'] = df['education'].apply(normalize_text)
    
    # Create mappings for education levels
    education_mappings = {
        'high school|secondary|ged': 'high_school',
        'college|university|undergrad|bachelor|associate': 'bachelors',
        'master|graduate|mba|ms |ma ': 'masters',
        'phd|doctorate|doctor|jd|md ': 'doctorate',
        'dropped out|some college|some university': 'some_college',
        'trade|vocational|technical': 'vocational',
        'school of hard knocks|university of life': 'alternative',
        'space camp': 'alternative',
        'med school|medical school': 'medical_school',
        'law school': 'law_school'
    }
    
    # Apply mappings
    standardized = df['education_standardized'].copy()
    for pattern, category in education_mappings.items():
        mask = standardized.str.contains(pattern, case=False, na=False, regex=True)
        standardized.loc[mask] = category
    
    # If any education wasn't matched by the patterns, mark as 'other'
    standardized.fillna('other', inplace=True)
    
    # Update the dataframe
    df['education_level'] = standardized
    
    return df


# Apply both standardization functions
df_imputed = standardize_job_categories(df_imputed)
df_imputed = standardize_education_categories(df_imputed)

# Verify the results
print("Job Categories Distribution:")
print(df_imputed['job_category'].value_counts())

print("\nEducation Level Distribution:")
print(df_imputed['education_level'].value_counts())

# Plot the distributions
plt.figure(figsize=(12, 6))
df_imputed['job_category'].value_counts().head(15).plot(kind='bar')
plt.title('Top 15 Standardized Job Categories')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
df_imputed['education_level'].value_counts().plot(kind='bar')
plt.title('Standardized Education Levels')
plt.tight_layout()
plt.show()


nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')


def preprocess_essays(df, essay_cols=None):
    """
    Preprocess essay text data and extract features
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame containing essay columns
    essay_cols : list, optional
        List of essay column names to process
        
    Returns:
    --------
    pandas DataFrame
        DataFrame with additional essay features
    """
    processed_df = df.copy()

    if essay_cols is None:
        essay_cols = [col for col in df.columns if col.startswith('essay') and not col.endswith('_length')]

    # Initialize NLTK resources once
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    def clean_essay(text):
        if not isinstance(text, str) or text.strip() == '':
            return ''
        
        try:
            # Use more robust HTML parsing with error handling
            text = BeautifulSoup(text, 'html.parser').get_text()
        except Exception as e:
            # Just keep going if BeautifulSoup fails
            print(f"BeautifulSoup error: {e}")
        
        # Clean text
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)
        text = re.sub(r'[^\w\s.,!?]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    # Process each essay column
    for col in essay_cols:
        print(f"Processing {col}...")
        
        # Clean text
        processed_df[f'{col}_clean'] = processed_df[col].apply(clean_essay)
        
        # Initialize feature lists
        word_counts, sentence_counts, avg_word_lengths = [], [], []
        flesch_scores, lemmatized_texts, token_lists = [], [], []
        
        # Process each essay
        for text in processed_df[f'{col}_clean']:
            if not isinstance(text, str) or text.strip() == '':
                # Handle empty or invalid text
                word_counts.append(0)
                sentence_counts.append(0)
                avg_word_lengths.append(0)
                flesch_scores.append(0)
                lemmatized_texts.append('')
                token_lists.append([])
                continue
                
            # Tokenize and analyze
            tokens = word_tokenize(text.lower())
            filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]
            sentences = sent_tokenize(text)
            
            # Calculate metrics
            word_count = len(tokens)
            sentence_count = len(sentences)
            avg_word_length = sum(len(word) for word in tokens) / max(1, word_count)
            
            # Calculate readability with error handling
            try:
                flesch_score = textstat.flesch_reading_ease(text)
            except Exception:
                flesch_score = 0
                
            # Append to feature lists
            word_counts.append(word_count)
            sentence_counts.append(sentence_count)
            avg_word_lengths.append(avg_word_length)
            flesch_scores.append(flesch_score)
            lemmatized_texts.append(' '.join(filtered_tokens))
            token_lists.append(filtered_tokens)
            
        # Add features to dataframe
        processed_df[f'{col}_word_count'] = word_counts
        processed_df[f'{col}_sentence_count'] = sentence_counts
        processed_df[f'{col}_avg_word_length'] = avg_word_lengths
        processed_df[f'{col}_flesch_reading_ease'] = flesch_scores
        processed_df[f'{col}_lemmatized'] = lemmatized_texts
        processed_df[f'{col}_tokens'] = token_lists
    
    # Calculate aggregate metrics
    processed_df['total_essay_word_count'] = processed_df[[f'{col}_word_count' for col in essay_cols]].sum(axis=1)
    processed_df['avg_essay_reading_ease'] = processed_df[[f'{col}_flesch_reading_ease' for col in essay_cols]].mean(axis=1)
    
    # Calculate essay completion metrics
    completed_essays = (processed_df[[f'{col}_word_count' for col in essay_cols]] > 0).sum(axis=1)
    processed_df['essay_completion_count'] = completed_essays
    processed_df['essay_completion_rate'] = completed_essays / len(essay_cols)
    
    return processed_df


def extract_topics_and_entities(df, essay_cols=None, max_rows=None):
    """
    Extract topics and named entities from preprocessed essays
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame with preprocessed essays
    essay_cols : list, optional
        List of essay column names to process
    max_rows : int, optional
        Maximum number of rows to process (for debugging)
        
    Returns:
    --------
    pandas DataFrame
        DataFrame with additional topic and entity features
    """
    # If essay columns not specified, find all columns starting with 'essay'
    if essay_cols is None:
        essay_cols = [col for col in df.columns if col.startswith('essay') and not col.endswith(('_clean', '_lemmatized', '_tokens'))]
    
    result_df = df.copy()
    
    # Limit rows for debugging if specified
    if max_rows is not None:
        result_df = result_df.head(max_rows)
    
    # Common topics in dating profiles
    topic_keywords = {
        'travel': ['travel', 'journey', 'trip', 'adventure', 'abroad', 'backpack', 'explore', 'wander', 'vacation', 'destination'],
        'food': ['food', 'eat', 'restaurant', 'cook', 'cuisine', 'recipe', 'dinner', 'lunch', 'breakfast', 'meal', 'chef', 'bake'],
        'fitness': ['gym', 'workout', 'exercise', 'fit', 'run', 'yoga', 'hike', 'climb', 'swim', 'bike', 'marathon', 'training'],
        'music': ['music', 'band', 'concert', 'song', 'album', 'play', 'guitar', 'piano', 'singing', 'singer'],
        'movies': ['movie', 'film', 'cinema', 'theater', 'actress', 'actor', 'director', 'watch', 'hollywood', 'show', 'series'],
        'books': ['book', 'read', 'author', 'novel', 'story', 'literature', 'fiction', 'nonfiction', 'chapter', 'library'],
        'art': ['art', 'paint', 'draw', 'artist', 'museum', 'gallery', 'creative', 'design', 'photography'],
        'technology': ['tech', 'computer', 'software', 'hardware', 'programming', 'code', 'developer', 'internet', 'digital', 'startup'],
        'family': ['family', 'parent', 'child', 'kid', 'mom', 'dad', 'mother', 'father', 'sister', 'brother', 'sibling'],
        'education': ['school', 'college', 'university', 'degree', 'student', 'learn', 'study', 'class', 'education', 'academic']
    }
    
    # Process each essay to identify topics
    for col in essay_cols:
        clean_col = f'{col}_clean'
        if clean_col not in result_df.columns:
            print(f"Warning: {clean_col} not found. Run preprocess_essays first.")
            continue
        
        # Identify topics - more efficient approach
        for topic, keywords in topic_keywords.items():
            # Create a set of keywords for faster lookup
            keyword_set = set(keywords)
            result_df[f'{col}_{topic}_score'] = result_df[clean_col].apply(
                lambda x: sum(word in keyword_set for word in x.lower().split()) if isinstance(x, str) and x else 0
            )
    
    # Create aggregate topic scores
    for topic in topic_keywords.keys():
        topic_cols = [col for col in result_df.columns if col.endswith(f'_{topic}_score')]
        if topic_cols:
            result_df[f'total_{topic}_score'] = result_df[topic_cols].sum(axis=1)
    
    return result_df

# Main execution function with progress tracking and error handling
def process_essay_data(df, essay_cols=None, max_rows=None, skip_entity_extraction=True):
    """
    Process essay data with progress tracking and error handling
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame containing essay columns
    essay_cols : list, optional
        List of essay column names to process
    max_rows : int, optional
        Maximum number of rows to process (for debugging)
    skip_entity_extraction : bool, default=True
        Whether to skip the entity extraction (which can be slow)
        
    Returns:
    --------
    pandas DataFrame
        DataFrame with additional essay features
    """
    try:
        # Download all necessary NLTK resources at once
        resources = ['punkt', 'stopwords', 'wordnet']
        if not skip_entity_extraction:
            resources.extend(['averaged_perceptron_tagger', 'maxent_ne_chunker', 'words'])
            
        for resource in resources:
            try:
                nltk.download(resource, quiet=True)
            except Exception as e:
                print(f"Error downloading {resource}: {e}")
        
        print("NLTK resources downloaded successfully")
        
        # Process essay columns - ensure we have no duplicates
        if essay_cols is None:
            essay_cols = [col for col in df.columns if col.startswith('essay') and not col.endswith(('_length', '_clean', '_lemmatized', '_tokens'))]
        
        print(f"Processing {len(essay_cols)} essay columns")
        
        # Remove duplicates from dataframe if any exist
        df = df.loc[:, ~df.columns.duplicated()]
        
        # Limit rows for debugging if specified
        if max_rows is not None:
            df = df.head(max_rows)
            print(f"Limited to {max_rows} rows for testing")
        
        # Apply text preprocessing pipeline
        print("Starting essay preprocessing...")
        df_processed = preprocess_essays(df, essay_cols)
        print("Essay preprocessing completed")
        
        # Extract topics (and entities if not skipped)
        print("Starting topic extraction...")
        df_processed = extract_topics_and_entities(df_processed, essay_cols, max_rows)
        print("Topic extraction completed")
        
        # Display summary of text features
        text_feature_cols = [col for col in df_processed.columns if any(col.endswith(suffix) for suffix in ['_word_count', '_reading_ease', '_score'])]
        print("\nText Feature Summary:")
        print(df_processed[text_feature_cols].describe().T.sort_values('mean', ascending=False).head(10))
        
        return df_processed
        
    except Exception as e:
        print(f"Error in essay processing: {str(e)}")
        import traceback
        traceback.print_exc()
        return df



# use the process_essay_data function
# which consolidates all the steps with better error handling and progress tracking
df_processed = process_essay_data(
    df=df_imputed,
    essay_cols=None,  # Will automatically find essay columns
    max_rows=None,    # Process all rows (set a number for testing)
    skip_entity_extraction=False  # Set to True if entity extraction is too slow
)

# If you want more control over individual steps, you can use this approach instead:
"""
# Download all necessary NLTK resources at once
resources = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words']
for resource in resources:
    try:
        nltk.download(resource, quiet=True)
    except Exception as e:
        print(f"Error downloading {resource}: {e}")

print("NLTK resources downloaded successfully")

# Process essay columns - ensure we have no duplicates
essay_cols = [col for col in df_imputed.columns if col.startswith('essay') and not col.endswith(('_length', '_clean', '_lemmatized', '_tokens'))]
print(f"Processing {len(essay_cols)} essay columns")

# Remove duplicates from dataframe if any exist
df_imputed = df_imputed.loc[:, ~df_imputed.columns.duplicated()]

# Apply text preprocessing pipeline to extract features
df_processed = preprocess_essays(df_imputed, essay_cols)
print("Essay preprocessing completed")

# Extract topics and named entities - added max_rows parameter
# You can set max_rows to a small number for testing
df_processed = extract_topics_and_entities(df_processed, essay_cols, max_rows=None)
print("Topic and entity extraction completed")
"""

# Display summary of text features
text_feature_cols = [col for col in df_processed.columns if any(col.endswith(suffix) for suffix in ['_word_count', '_reading_ease', '_score'])]
print("\nText Feature Summary:")
summary = df_processed[text_feature_cols].describe().T.sort_values('mean', ascending=False).head(10)
print(summary)

# Save the processed dataframe
df_processed.to_csv('processed_essays.csv', index=False)


df_processed.columns


print(df_processed['last_online'].head())


df_copy = df_processed.copy()
df_processed['last_online'] = df_processed['last_online'].apply(lambda date_time: datetime.strptime(date_time, '%Y-%m-%d-%H-%M'))
print(df_processed.head())


print(df_processed['last_online'])


############################################################################################################################################
############################################           Feature Engineering               ###################################################
############################################################################################################################################

# Demographic Features
print(df_processed['age'].head())


bins = list(range(int(df_processed['age'].min())//5 * 5, int(df_processed['age'].max())//5*5+5, 5))
df_processed['age'] = pd.cut(df_processed['age'].astype('int'), bins)

print(df_processed['age'].unique())


df_processed.columns


lifestyle_columns = ['body_type', 'diet', 'drinks', 'education', 'smokes']
df_processed.status.unique()


def create_lifestyle_indicators(df):
    """
    Create lifestyle composite indicators combining drinks, drugs, and smokes data
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame containing lifestyle columns (drinks, drugs, smokes)
    
    Returns:
    --------
    pandas DataFrame
        DataFrame with added lifestyle indicators
    """
    # Make a copy to avoid modifying the original dataframe
    result_df = df.copy()
    
    # Define mappings for each lifestyle attribute
    drinks_mapping = {
        'not at all': 0,
        'rarely': 1,
        'socially': 2,
        'often': 3,
        'very often': 4,
        'desperately': 5
    }
    
    drugs_mapping = {
        'never': 0,
        'sometimes': 1,
        'often': 2
    }
    
    smoking_mapping = {
        'no': 0,
        'sometimes': 1,
        'when drinking': 2,
        'trying to quit': 3,
        'yes': 4
    }
    
    # Apply mappings to create numerical indicators
    columns_to_process = ['drinks', 'drugs', 'smokes']
    mappings = {
        'drinks': drinks_mapping,
        'drugs': drugs_mapping,
        'smokes': smoking_mapping
    }
    
    # Create numerical versions of each column
    for col in columns_to_process:
        if col in result_df.columns:
            # Create new column with '_score' suffix
            score_col = f'{col}_score'
            result_df[score_col] = result_df[col].map(mappings[col])
            
            # Fill missing values with median of non-missing values
            median_value = result_df[score_col].median()
            if pd.isna(median_value):  # If all values are NA
                median_value = 0
            result_df[score_col] = result_df[score_col].fillna(median_value)
    
    # Calculate total lifestyle score
    lifestyle_columns = [f'{col}_score' for col in columns_to_process 
                         if f'{col}_score' in result_df.columns]
    
    if lifestyle_columns:
        # Sum up scores
        result_df['lifestyle_score'] = 0.2*result_df['drinks_score'] + 0.4*result_df['drugs_score'] + 0.4*result_df['smokes_score']
        #result_df[lifestyle_columns].sum(axis=1)
        
        # Create categorized version
        # Adjust bin boundaries based on actual range of scores
        max_score = sum([
            max(mappings[col].values()) if col in result_df.columns else 0
            for col in columns_to_process
        ])
        
        # Create logical bin boundaries
        bins = [0, max_score / 3, 2 * max_score / 3, max_score]
        labels = ['Conservative', 'Moderate', 'Liberal']
        
        result_df['lifestyle_category'] = pd.cut(
            result_df['lifestyle_score'], 
            bins=bins, 
            labels=labels,
            include_lowest=True
        )
        
        # Calculate normalized score (0-1 scale)
        result_df['lifestyle_index'] = result_df['lifestyle_score'] / max_score
    
    return result_df

def analyze_lifestyle_patterns(df):
    """
    Analyze patterns in lifestyle indicators across demographics
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame with lifestyle indicators
    """
    if 'lifestyle_category' not in df.columns:
        print("Run create_lifestyle_indicators first to generate lifestyle categories")
        return
    
    # Visualize lifestyle category distribution
    plt.figure(figsize=(10, 6))
    ax = sns.countplot(data=df, x='lifestyle_category')
    
    # Add count labels on top of bars
    for p in ax.patches:
        ax.annotate(f'{int(p.get_height())}', 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'bottom')
                   
    plt.title('Distribution of Lifestyle Categories')
    plt.xlabel('Lifestyle Category')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.show()
    
    # Analyze relationships with age
    if 'age' in df.columns:
        plt.figure(figsize=(12, 7))
        if hasattr(df['age'], 'cat') or df['age'].dtype.name == 'category':
            # Age is already categorized
            ct = pd.crosstab(df['age'], df['lifestyle_category'], normalize='index')
            ct.plot(kind='bar', stacked=True, figsize=(12, 7))
            plt.title('Lifestyle Categories by Age Group')
            plt.legend(title='Lifestyle', bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.xticks(rotation=45)
        else:
            # Create age groups if age is numeric
            sns.boxplot(x='lifestyle_category', y='age', data=df)
            plt.title('Age Distribution by Lifestyle Category')
        plt.tight_layout()
        plt.show()
    
    # Analyze relationships with sex
    if 'sex' in df.columns:
        plt.figure(figsize=(10, 6))
        cross_tab = pd.crosstab(df['sex'], df['lifestyle_category'])
        cross_tab_pct = cross_tab.div(cross_tab.sum(axis=1), axis=0)
        ax = cross_tab_pct.plot(kind='bar', figsize=(10, 6))
        
        # Add percentage labels
        for i, container in enumerate(ax.containers):
            ax.bar_label(container, fmt='%.1f%%', padding=3)
            
        plt.title('Lifestyle Categories by Gender')
        plt.ylabel('Percentage')
        plt.legend(title='Lifestyle', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()
    
    # Show average scores for each component - IMPROVED VISUALIZATION
    lifestyle_cols = [col for col in df.columns if col.endswith('_score') and col != 'lifestyle_score']
    if lifestyle_cols:
        # Increase figure height to accommodate labels
        plt.figure(figsize=(10, max(6, len(lifestyle_cols) * 0.5)))
        
        # Get mean values and sort
        mean_scores = df[lifestyle_cols].mean().sort_values()
        
        # Create custom bar chart with better spacing
        ax = mean_scores.plot(kind='barh', color='steelblue')
        
        # Clean up the y-axis labels (remove "_score" suffix for readability)
        labels = [label.replace('_score', '') for label in mean_scores.index]
        ax.set_yticklabels(labels)
        
        # Add value labels at the end of each bar
        for i, v in enumerate(mean_scores):
            ax.text(v + 0.05, i, f'{v:.2f}', va='center')
        
        plt.title('Average Scores for Lifestyle Components')
        plt.xlabel('Average Score')
        
        # Add grid lines for better readability
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        
        # Ensure proper spacing
        plt.tight_layout()
        plt.show()


new_df = create_lifestyle_indicators(df_processed)
analyze_lifestyle_patterns(new_df)


new_df['job'].unique()


def create_socioeconomic_indicators(df):
    """
    Create socioeconomic composite indicators combining education, job, and income data
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame containing education, job, and income columns
    
    Returns:
    --------
    pandas DataFrame
        DataFrame with added socioeconomic indicators
    """
    # Make a copy to avoid modifying the original dataframe
    result_df = df.copy()
    
    # Define mappings for education levels
    education_mapping = {
        'dropped out of high school': 0,
        'working on high school': 1,
        'high school': 2,
        'graduated from high school': 2,
        'dropped out of two-year college': 3,
        'two-year college': 4,
        'working on two-year college': 4,
        'dropped out of college/university': 4,
        'graduated from two-year college': 5,
        'working on college/university': 6,
        'college/university': 6,
        'dropped out of law school': 6,
        'dropped out of med school': 6,
        'dropped out of masters program': 6,
        'graduated from college/university': 7,
        'working on law school': 7,
        'working on med school': 7,
        'working on masters program': 7,
        'law school': 7,
        'med school': 7,
        'masters program': 7,
        'graduated from law school': 8,
        'graduated from med school': 8,
        'graduated from masters program': 8,
        'dropped out of ph.d program': 8,
        'working on ph.d program': 9,
        'ph.d program': 9,
        'graduated from ph.d program': 10
    }

    # Define mappings for job categories
    job_mapping = {
        'unemployed': 0,
        'retired': 1,
        'student': 2,
        'clerical / administrative': 3,
        'transportation': 3,
        'hospitality / travel': 3,
        'construction / craftsmanship': 4,
        'military': 4,
        'other': 5,
        'artistic / musical / writer': 5,
        'entertainment / media': 5,
        'sales / marketing / biz dev': 6,
        'education / academia': 6,
        'political / government': 6,
        'law / legal services': 7,
        'medicine / health': 8,
        'banking / financial / real estate': 8,
        'science / tech / engineering': 8,
        'computer / hardware / software': 8,
        'executive / management': 9
    }

    # Define columns to process
    columns_to_process = ['education', 'job', 'income']
    mappings = {
        'education': education_mapping,
        'job': job_mapping
    }
    
    # Create numerical versions of categorical columns
    for col in ['education', 'job']:
        if col in result_df.columns:
            # Create new column with '_score' suffix
            score_col = f'{col}_score'
            result_df[score_col] = result_df[col].map(mappings[col])
            
            # Fill missing values with median of non-missing values
            median_value = result_df[score_col].median()
            if pd.isna(median_value):  # If all values are NA
                median_value = 0
            result_df[score_col] = result_df[score_col].fillna(median_value)

    # Process income separately - normalize to a 0-10 scale to match other metrics
    if 'income' in result_df.columns:
        # Cap income at 250,000 to prevent outliers from skewing the scale
        capped_income = result_df['income'].clip(upper=250000)
        # Scale to 0-10 range
        result_df['income_score'] = (capped_income / 25000).clip(upper=10)
        # Fill missing values with median
        result_df['income_score'] = result_df['income_score'].fillna(result_df['income_score'].median())
    
    # Calculate socioeconomic indicator with weighted components
    socioeconomic_columns = ['education_score', 'job_score', 'income_score']
    
    # Check if we have all the required columns
    if all(col in result_df.columns for col in socioeconomic_columns):
        # Calculate weighted score - education (30%), job (30%), income (40%)
        result_df['socioeconomic_score'] = (
            0.3 * result_df['education_score'] + 
            0.3 * result_df['job_score'] + 
            0.4 * result_df['income_score']
        )
        
        # Create categorized version
        # Define reasonable bin boundaries for the combined score
        bins = [0, 3, 6, 10]  # Adjusted for the weighted scale
        labels = ['Lower', 'Middle', 'Upper']
        
        result_df['socioeconomic_category'] = pd.cut(
            result_df['socioeconomic_score'], 
            bins=bins, 
            labels=labels,
            include_lowest=True
        )
        
        # Calculate normalized score (0-1 scale)
        result_df['socioeconomic_index'] = result_df['socioeconomic_score'] / 10
    
    return result_df

def analyze_socioeconomic_patterns(df):
    """
    Analyze patterns in socioeconomic indicators across demographics
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame with socioeconomic indicators
    """
    if 'socioeconomic_category' not in df.columns:
        print("Run create_socioeconomic_indicators first to generate categories")
        return
    
    # Visualize socioeconomic category distribution
    plt.figure(figsize=(10, 6))
    ax = sns.countplot(data=df, x='socioeconomic_category')
    
    # Add count labels on top of bars
    for p in ax.patches:
        ax.annotate(f'{int(p.get_height())}', 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'bottom')
                   
    plt.title('Distribution of Socioeconomic Categories')
    plt.xlabel('Socioeconomic Category')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.show()
    
    # Analyze relationships with age
    if 'age' in df.columns:
        plt.figure(figsize=(12, 7))
        if hasattr(df['age'], 'cat') or df['age'].dtype.name == 'category':
            # Age is already categorized
            ct = pd.crosstab(df['age'], df['socioeconomic_category'], normalize='index')
            ct.plot(kind='bar', stacked=True, figsize=(12, 7))
            plt.title('Socioeconomic Categories by Age Group')
            plt.legend(title='Socioeconomic', bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.xticks(rotation=45)
        else:
            # Create age groups if age is numeric
            sns.boxplot(x='socioeconomic_category', y='age', data=df)
            plt.title('Age Distribution by Socioeconomic Category')
        plt.tight_layout()
        plt.show()
    
    # Analyze relationships with sex
    if 'sex' in df.columns:
        plt.figure(figsize=(10, 6))
        cross_tab = pd.crosstab(df['sex'], df['socioeconomic_category'])
        cross_tab_pct = cross_tab.div(cross_tab.sum(axis=1), axis=0)
        ax = cross_tab_pct.plot(kind='bar', figsize=(10, 6))
        
        # Add percentage labels
        for i, container in enumerate(ax.containers):
            ax.bar_label(container, fmt='%.1f%%', padding=3)
            
        plt.title('Socioeconomic Categories by Gender')
        plt.ylabel('Percentage')
        plt.legend(title='Socioeconomic', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()
    
    # Show average scores for each component
    socioeconomic_cols = ['education_score', 'job_score', 'income_score']
    if all(col in df.columns for col in socioeconomic_cols):
        # Increase figure height to accommodate labels
        plt.figure(figsize=(10, 5))
        
        # Get mean values and sort
        mean_scores = df[socioeconomic_cols].mean().sort_values()
        
        # Create custom bar chart with better spacing
        ax = mean_scores.plot(kind='barh', color='steelblue')
        
        # Clean up the y-axis labels
        labels = [label.replace('_score', '') for label in mean_scores.index]
        ax.set_yticklabels(labels, fontsize=12)
        
        # Add value labels at the end of each bar
        for i, v in enumerate(mean_scores):
            ax.text(v + 0.05, i, f'{v:.2f}', va='center')
        
        plt.title('Average Scores for Socioeconomic Components')
        plt.xlabel('Average Score')
        
        # Add grid lines for better readability
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        
        # Ensure proper spacing
        plt.tight_layout()
        plt.show()
        
        # Show correlation between components
        plt.figure(figsize=(8, 6))
        sns.heatmap(df[socioeconomic_cols + ['socioeconomic_score']].corr(), 
                    annot=True, cmap='coolwarm', fmt='.2f')
        plt.title('Correlation Between Socioeconomic Components')
        plt.tight_layout()
        plt.show()


new_df = create_socioeconomic_indicators(new_df)
analyze_socioeconomic_patterns(new_df)


new_df['socioeconomic_score'].head()


new_df['offspring'].unique()


def build_relationship_indicators(df):
    """
    Create relationship indicators based on status and offspring information
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame containing status and offspring columns
    
    Returns:
    --------
    pandas DataFrame
        DataFrame with added relationship indicators
    """
    # Make a copy to avoid modifying the original dataframe
    result_df = df.copy()
    
    # Define mappings for relationship status
    status_mapping = {
        'single': 0,
        'available': 1,
        'seeing someone': 2,
        'married': 3
    }
    
    # Define mappings for offspring preferences
    offspring_mapping = {
        'doesn&rsquo;t have kids, and doesn&rsquo;t want any': 0,
        'doesn&rsquo;t want kids': 0,
        'doesn&rsquo;t have kids, but might want them': 1,
        'might want kids': 1,
        'doesn&rsquo;t have kids': 1,
        'doesn&rsquo;t have kids, but wants them': 2,
        'wants kids': 2,
        'has a kid, but doesn&rsquo;t want more': 3,
        'has a kid, and might want more': 4,
        'has a kid': 4,
        'has a kid, and wants more': 5,
        'has kids, but doesn&rsquo;t want more': 6,
        'has kids, and might want more': 7,
        'has kids': 7,
        'has kids, and wants more': 8
    }
    
    # Create numerical versions of each column
    if 'status' in result_df.columns:
        result_df['status_score'] = result_df['status'].map(status_mapping)
        # Fill missing values with median
        median_status = result_df['status_score'].median()
        result_df['status_score'] = result_df['status_score'].fillna(median_status)
    
    if 'offspring' in result_df.columns:
        result_df['offspring_score'] = result_df['offspring'].map(offspring_mapping)
        # Fill missing values with median
        median_offspring = result_df['offspring_score'].median()
        result_df['offspring_score'] = result_df['offspring_score'].fillna(median_offspring)
    
    # Create categorical features for offspring status
    if 'offspring' in result_df.columns:
        # Has kids or not
        result_df['has_children'] = result_df['offspring'].str.contains('has').fillna(False)
        
        # Wants kids or not
        result_df['wants_children'] = (
            result_df['offspring'].str.contains('wants|might want').fillna(False) & 
            ~result_df['offspring'].str.contains('doesn').fillna(False)
        )
        
        # Create a composite offspring category
        def get_offspring_category(row):
            if pd.isna(row['offspring']):
                return 'Unknown'
            elif 'has' in row['offspring']:
                if 'more' in row['offspring']:
                    if 'doesn' in row['offspring']:
                        return 'Has children, no more'
                    else:
                        return 'Has children, wants more'
                else:
                    return 'Has children'
            else:
                if 'doesn' in row['offspring'] and 'want any' in row['offspring']:
                    return 'No children, wants none'
                elif 'wants' in row['offspring']:
                    return 'No children, wants some'
                elif 'might' in row['offspring']:
                    return 'No children, might want some'
                else:
                    return 'No children, unknown preference'
        
        result_df['offspring_category'] = result_df.apply(get_offspring_category, axis=1)
        
        # Create a family orientation category
        def get_family_orientation(row):
            if pd.isna(row['offspring']):
                return 'Unknown'
            elif 'doesn' in row['offspring'] and 'want any' in row['offspring']:
                return 'Not family-oriented'
            elif 'doesn' in row['offspring'] and 'want more' in row['offspring']:
                return 'Moderately family-oriented'
            elif 'wants' in row['offspring'] or 'might want' in row['offspring'] or 'wants more' in row['offspring']:
                return 'Family-oriented'
            else:
                return 'Neutral'
        
        result_df['family_orientation_category'] = result_df.apply(get_family_orientation, axis=1)
    
    # Create a relationship availability indicator
    if all(col in result_df.columns for col in ['status_score', 'offspring_score']):
        # Define availability score based on status and how complicated offspring situation is
        # Lower score = more available and less complicated
        result_df['relationship_availability'] = (
            (3 - result_df['status_score']) * 0.7 +  # Invert status so single = high score
            (1 / (1 + result_df['offspring_score'])) * 0.3  # Higher offspring score = lower availability
        )
        
        # Normalize to 0-1 scale
        min_avail = result_df['relationship_availability'].min()
        max_avail = result_df['relationship_availability'].max()
        result_df['relationship_availability'] = (
            (result_df['relationship_availability'] - min_avail) / (max_avail - min_avail)
        )
        
        # Create relationship availability categories
        bins = [0, 0.33, 0.66, 1]
        labels = ['Low Availability', 'Medium Availability', 'High Availability']
        result_df['availability_category'] = pd.cut(
            result_df['relationship_availability'],
            bins=bins,
            labels=labels,
            include_lowest=True
        )
    
    return result_df

def analyze_relationship_patterns(df):
    """
    Analyze patterns in relationship indicators
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame with relationship indicators
    """
    # Check if we have the necessary columns
    required_cols = ['status_score', 'offspring_score', 'offspring_category', 'availability_category']
    if not all(col in df.columns for col in required_cols):
        print("Run build_relationship_indicators first to generate relationship indicators")
        return
    
    # Visualize status distribution
    if 'status' in df.columns:
        plt.figure(figsize=(10, 6))
        sns.countplot(data=df, x='status', order=df['status'].value_counts().index)
        plt.title('Distribution of Relationship Status')
        plt.xlabel('Status')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
    
    # Visualize offspring category distribution
    plt.figure(figsize=(12, 6))
    offspring_order = df['offspring_category'].value_counts().index
    sns.countplot(data=df, x='offspring_category', order=offspring_order)
    plt.title('Distribution of Offspring Categories')
    plt.xlabel('Offspring Category')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    # Visualize availability categories
    plt.figure(figsize=(10, 6))
    ax = sns.countplot(data=df, x='availability_category')
    
    # Add count labels on top of bars
    for p in ax.patches:
        ax.annotate(f'{int(p.get_height())}', 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'bottom')
    
    plt.title('Distribution of Relationship Availability')
    plt.tight_layout()
    plt.show()
    
    # Analyze patterns by gender
    if 'sex' in df.columns:
        plt.figure(figsize=(12, 8))
        
        # Cross-tabulation of status by gender
        cross_tab = pd.crosstab(df['sex'], df['status'])
        cross_tab_pct = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100
        
        ax = cross_tab_pct.plot(kind='bar', figsize=(12, 6))
        plt.title('Relationship Status by Gender')
        plt.ylabel('Percentage')
        plt.legend(title='Status')
        plt.tight_layout()
        plt.show()
        
        # Cross-tabulation of offspring preference by gender
        plt.figure(figsize=(14, 8))
        offspring_cross = pd.crosstab(df['sex'], df['offspring_category'])
        offspring_pct = offspring_cross.div(offspring_cross.sum(axis=1), axis=0) * 100
        
        offspring_pct.plot(kind='bar', figsize=(14, 6))
        plt.title('Offspring Preferences by Gender')
        plt.ylabel('Percentage')
        plt.legend(title='Offspring Category', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()
    
    # Analyze relationship with age
    if 'age' in df.columns and df['age'].dtype.name == 'category':
        # Age vs. offspring category
        plt.figure(figsize=(16, 8))
        ct_offspring = pd.crosstab(df['age'], df['offspring_category'], normalize='index') * 100
        ct_offspring.plot(kind='bar', stacked=True)
        plt.title('Offspring Preferences by Age Group')
        plt.ylabel('Percentage')
        plt.legend(title='Offspring Category', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        
        # Age vs. relationship status
        plt.figure(figsize=(14, 8))
        ct_status = pd.crosstab(df['age'], df['status'], normalize='index') * 100
        ct_status.plot(kind='bar', stacked=True)
        plt.title('Relationship Status by Age Group')
        plt.ylabel('Percentage')
        plt.legend(title='Status')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
    
    # Analyze relationship between availability and other indicators
    if 'lifestyle_category' in df.columns:
        plt.figure(figsize=(12, 6))
        lifestyle_avail = pd.crosstab(df['lifestyle_category'], df['availability_category'], normalize='index') * 100
        lifestyle_avail.plot(kind='bar', stacked=False)
        plt.title('Relationship Availability by Lifestyle Category')
        plt.ylabel('Percentage')
        plt.legend(title='Availability')
        plt.tight_layout()
        plt.show()


# Apply the function to create relationship indicators
df_comp = build_relationship_indicators(new_df)

# Run the analysis function
analyze_relationship_patterns(df_comp)

# Additional analysis - cross-tabulate with socioeconomic indicators
plt.figure(figsize=(12, 6))
rel_socio = pd.crosstab(
    df_comp['socioeconomic_category'], 
    df_comp['availability_category'],
    normalize='index'
) * 100
rel_socio.plot(kind='bar')
plt.title('Relationship Availability by Socioeconomic Status')
plt.ylabel('Percentage')
plt.xlabel('Socioeconomic Category')
plt.legend(title='Availability')
plt.tight_layout()
plt.show()

# Analyze family orientation by age
if 'family_orientation_category' in df_comp.columns:
    plt.figure(figsize=(12, 6))
    family_age = pd.crosstab(
        df_comp['age'],
        df_comp['family_orientation_category'],
        normalize='index'
    ) * 100
    family_age.plot(kind='bar', stacked=True)
    plt.title('Family Orientation by Age Group')
    plt.ylabel('Percentage')
    plt.xlabel('Age Group')
    plt.legend(title='Family Orientation', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()


for col in df_comp.columns:
    print(col)


# Text-based features
def map_essays_by_topic(df, essay_cols=None, n_topics=5, max_features=10000, random_state=42):
    """
    Classify essays based on their content for various tasks
    
    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame containing processed essay columns
    task : str, default='topic'
        Classification task to perform:
        - 'topic': Classify essays into topics
        - 'sentiment': Analyze sentiment of essays
     - 'personality': Detect personality traits
        - 'style': Analyze writing style
        - 'custom': Use a custom target column for supervised learning
    essays_cols : list, default=None
        List of essay columns to use. If None, use all columns starting with 'essay' 
        and ending with '_clean' or '_lemmatized'
    target_col : str, default=None
        Target column for supervised classification (required for 'custom' task)
    model_type : str, default='tfidf_nb'
        Model type to use:
        - 'tfidf_nb': TF-IDF + Naive Bayes
        - 'tfidf_svm': TF-IDF + SVM
        - 'bow_nb': Bag of Words + Naive Bayes
        - 'bert': BERT-based model (requires transformers package)
        - 'lda': Latent Dirichlet Allocation (unsupervised topic modeling)
    n_topics : int, default=5
        Number of topics for unsupervised topic modeling
    custom_labels : list, default=None
        Custom labels for classification (required for some tasks)
    test_size : float, default=0.2
        Proportion of data to use for testing
    random_state : int, default=42
        Random seed for reproducibility
    
    Returns:
    --------
    dict
        Dictionary containing model, predictions, evaluation metrics, and transformed dataframe
    """

    result_df = df.copy
        
    essay_cols = [col for col in df.columns if col.startswith('essay') and 
                      (col.endswith('_clean') or col.endswith('_lemmatized'))]
    
    print(f"Using {len(essay_cols)} essay columns for analysis")
    df['combined_essays'] = df[essay_cols].apply(
        lambda row: ' '.join([str(text) for text in row if isinstance(text, str) and text.strip()]),
        axis=1
    )

    valid_essays = df['combined_essays'].str.strip() != ''
    print(f"Found {valid_essays.sum()} users with non-empty essays")

    # Unsupervised topic modeling with LDA
    print(f"Performing unsupervised topic modeling with {n_topics} topics")
        
    vectorizer = CountVectorizer(max_df=0.95, min_df=2, 
                                    max_features=max_features, 
                                    stop_words='english')
    X = vectorizer.fit_transform(df.loc[valid_essays, 'combined_essays'])

    lda = LatentDirichletAllocation(n_components=n_topics, 
                                       learning_method='online',
                                       max_iter=20, 
                                       random_state=random_state)
    topic_distributions = lda.fit_transform(X)

    # Get top words for each topic
    feature_names = vectorizer.get_feature_names_out()
    n_top_words = 10
    topic_keywords = []
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_words_idx]
        topic_keywords.append(top_words)
        print(f"Topic #{topic_idx + 1}: {', '.join(top_words)}")

        # Assign dominant topic to each document
    dominant_topic = np.argmax(topic_distributions, axis=1)

        # Add results to dataframe
    df.loc[valid_essays, 'dominant_topic'] = dominant_topic + 1  # 1-based indexing
        # Add topic distributions
    for i in range(n_topics):
        df.loc[valid_essays, f'topic_{i+1}_score'] = topic_distributions[:, i]

    return {
        'model': lda,
        'vectorizer': vectorizer,
        'topic_keywords': topic_keywords,
        'df': df,
        'topic_distributions': topic_distributions
    }
    


topic_results = map_essays_by_topic(
    df_comp,
    n_topics=8
)

for i, keywords in enumerate(topic_results['topic_keywords']):
    print(f"Topic {i+1}: {', '.join(keywords)}")



plt.figure(figsize=(12, 6))
sns.countplot(data=topic_results['df'], x='dominant_topic')
plt.title('Distribution of Dominant Topics')
plt.xlabel('Topic Number')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


full_df = topic_results['df']
full_df.head()


def extract_named_entities(text):
    result_df = df.copy()
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    return [(ent.text, ent.label_) for ent in doc.ents]

